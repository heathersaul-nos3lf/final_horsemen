{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Download library--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: corextopic in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.0.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install corextopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--import dependencies--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_lg\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--read in data--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_anx = pd.read_csv(r'2020_March_r_Anxiety.csv')\n",
    "doc_anx = doc_anx[doc_anx.Body != '[removed]']\n",
    "\n",
    "text_doc_anx = doc_anx['Body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_anx = []\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in text_doc_anx:\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(str(doc))\n",
    "    doc_list_anx.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>thing anxious focus mean study like maybe lazy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>know midnight know homework know school know X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>med euphoric feeling consider narcotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>heart feel sense yesterday continually worried...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fear swallow small bone eat chicken sure bite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7149</td>\n",
       "      <td>want sell ps4 use anymore want money hard pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>yesterday(this morning sleep midnight 12 wake ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7151</td>\n",
       "      <td>bath new house cause flood basement live boyfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7152</td>\n",
       "      <td>mean find self cross leg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7153</td>\n",
       "      <td>start online class day day control announce on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7154 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Body\n",
       "0     thing anxious focus mean study like maybe lazy...\n",
       "1     know midnight know homework know school know X...\n",
       "2                med euphoric feeling consider narcotic\n",
       "3     heart feel sense yesterday continually worried...\n",
       "4     fear swallow small bone eat chicken sure bite ...\n",
       "...                                                 ...\n",
       "7149  want sell ps4 use anymore want money hard pers...\n",
       "7150  yesterday(this morning sleep midnight 12 wake ...\n",
       "7151  bath new house cause flood basement live boyfr...\n",
       "7152                           mean find self cross leg\n",
       "7153  start online class day day control announce on...\n",
       "\n",
       "[7154 rows x 1 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "empt = [];\n",
    "for doc in doc_list_anx:\n",
    "    a1 = TreebankWordDetokenizer().detokenize(doc)\n",
    "    empt.append(a1)\n",
    "    \n",
    "df_doc_anx = pd.DataFrame(empt,columns = ['Body'])\n",
    "#df_doc is a dataframe that has cleaned posts. \n",
    "#all the 'removed' posts are gone and all the stopwords in the individual posts are gone! \n",
    "\n",
    "#both doc_list and df_doc are important (at least I think so :D)\n",
    "\n",
    "\n",
    "df_doc_anx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>thing anxious focus mean study like maybe lazy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>know midnight know homework know school know X...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>med euphoric feeling consider narcotic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>heart feel sense yesterday continually worried...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fear swallow small bone eat chicken sure bite ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7149</td>\n",
       "      <td>want sell ps4 use anymore want money hard pers...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>yesterday(this morning sleep midnight 12 wake ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7151</td>\n",
       "      <td>bath new house cause flood basement live boyfr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7152</td>\n",
       "      <td>mean find self cross leg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7153</td>\n",
       "      <td>start online class day day control announce on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Body  label\n",
       "0     thing anxious focus mean study like maybe lazy...      1\n",
       "1     know midnight know homework know school know X...      1\n",
       "2                med euphoric feeling consider narcotic      1\n",
       "3     heart feel sense yesterday continually worried...      1\n",
       "4     fear swallow small bone eat chicken sure bite ...      1\n",
       "...                                                 ...    ...\n",
       "7149  want sell ps4 use anymore want money hard pers...      1\n",
       "7150  yesterday(this morning sleep midnight 12 wake ...      1\n",
       "7151  bath new house cause flood basement live boyfr...      1\n",
       "7152                           mean find self cross leg      1\n",
       "7153  start online class day day control announce on...      1\n",
       "\n",
       "[7154 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doc_anx['label']=1\n",
    "df_doc_anx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.read_csv(r'Datasets/2020_March_r_Depression.csv')\n",
    "doc = doc[doc.Body != '[removed]']\n",
    "doc = doc.sample(n=7154) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of documents (list of lists) \n",
    "text_doc = doc['Body'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en\")\n",
    "\n",
    "# List of stop words to equalize data\n",
    "stop_list = [\"Depression\",\"depression\",  \"anxiety\", \"Anxiety\"]\n",
    "\n",
    "# Updates spaCy's default stop words list with my additional words. \n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in text_doc:\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(str(doc))\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>thank come ted talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cry thing heavy feel like cry company rude lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>honestly lazy motivated life better like want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>day know reason live exist feel like fall endl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I‘ve depress 14 today 22nd birthday finally ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7149</td>\n",
       "      <td>feel like awful person rant know million peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>fuck literally die Jesus christ feel brain spl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7151</td>\n",
       "      <td>hate right stupid thing hate add Dyslexia Anzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7152</td>\n",
       "      <td>extremely happy able kill 15 min thought patte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7153</td>\n",
       "      <td>m mental breakdown t try eat drink water cry h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7154 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Body\n",
       "0                                   thank come ted talk\n",
       "1     cry thing heavy feel like cry company rude lea...\n",
       "2     honestly lazy motivated life better like want ...\n",
       "3     day know reason live exist feel like fall endl...\n",
       "4     I‘ve depress 14 today 22nd birthday finally ma...\n",
       "...                                                 ...\n",
       "7149  feel like awful person rant know million peopl...\n",
       "7150  fuck literally die Jesus christ feel brain spl...\n",
       "7151  hate right stupid thing hate add Dyslexia Anzi...\n",
       "7152  extremely happy able kill 15 min thought patte...\n",
       "7153  m mental breakdown t try eat drink water cry h...\n",
       "\n",
       "[7154 rows x 1 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#however doc_list gives us each post as a list, with individual words being elements \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "empt = [];\n",
    "for doc in doc_list:\n",
    "    a1 = TreebankWordDetokenizer().detokenize(doc)\n",
    "    empt.append(a1)\n",
    "    \n",
    "df_doc = pd.DataFrame(empt,columns = ['Body'])\n",
    "#df_doc is a dataframe that has cleaned posts. \n",
    "#all the 'removed' posts are gone and all the stopwords in the individual posts are gone! \n",
    "\n",
    "#both doc_list and df_doc are important (at least I think so :D)\n",
    "\n",
    "\n",
    "df_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>thank come ted talk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cry thing heavy feel like cry company rude lea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>honestly lazy motivated life better like want ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>day know reason live exist feel like fall endl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I‘ve depress 14 today 22nd birthday finally ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7149</td>\n",
       "      <td>feel like awful person rant know million peopl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>fuck literally die Jesus christ feel brain spl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7151</td>\n",
       "      <td>hate right stupid thing hate add Dyslexia Anzi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7152</td>\n",
       "      <td>extremely happy able kill 15 min thought patte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7153</td>\n",
       "      <td>m mental breakdown t try eat drink water cry h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Body  label\n",
       "0                                   thank come ted talk      0\n",
       "1     cry thing heavy feel like cry company rude lea...      0\n",
       "2     honestly lazy motivated life better like want ...      0\n",
       "3     day know reason live exist feel like fall endl...      0\n",
       "4     I‘ve depress 14 today 22nd birthday finally ma...      0\n",
       "...                                                 ...    ...\n",
       "7149  feel like awful person rant know million peopl...      0\n",
       "7150  fuck literally die Jesus christ feel brain spl...      0\n",
       "7151  hate right stupid thing hate add Dyslexia Anzi...      0\n",
       "7152  extremely happy able kill 15 min thought patte...      0\n",
       "7153  m mental breakdown t try eat drink water cry h...      0\n",
       "\n",
       "[7154 rows x 2 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doc['label'] = 0\n",
    "df_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>thank come ted talk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cry thing heavy feel like cry company rude lea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>honestly lazy motivated life better like want ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>day know reason live exist feel like fall endl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I‘ve depress 14 today 22nd birthday finally ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14303</td>\n",
       "      <td>want sell ps4 use anymore want money hard pers...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14304</td>\n",
       "      <td>yesterday(this morning sleep midnight 12 wake ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14305</td>\n",
       "      <td>bath new house cause flood basement live boyfr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14306</td>\n",
       "      <td>mean find self cross leg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14307</td>\n",
       "      <td>start online class day day control announce on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14308 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Body  label\n",
       "0                                    thank come ted talk      0\n",
       "1      cry thing heavy feel like cry company rude lea...      0\n",
       "2      honestly lazy motivated life better like want ...      0\n",
       "3      day know reason live exist feel like fall endl...      0\n",
       "4      I‘ve depress 14 today 22nd birthday finally ma...      0\n",
       "...                                                  ...    ...\n",
       "14303  want sell ps4 use anymore want money hard pers...      1\n",
       "14304  yesterday(this morning sleep midnight 12 wake ...      1\n",
       "14305  bath new house cause flood basement live boyfr...      1\n",
       "14306                           mean find self cross leg      1\n",
       "14307  start online class day day control announce on...      1\n",
       "\n",
       "[14308 rows x 2 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col_merged =pd.concat([df_doc, df_doc_anx], axis=0).reset_index(drop=True)\n",
    "df_col_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=.5,\n",
    "    min_df=10,\n",
    "    max_features=None,\n",
    "    ngram_range=(1, 1),\n",
    "    norm=None,\n",
    "    binary=True,\n",
    "    use_idf=False,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "\n",
    "vectorizer = vectorizer.fit(df_col_merged.Body)\n",
    "tfidf = vectorizer.transform(df_col_merged.Body)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14308x4547 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 576285 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corextopic import corextopic as ct\n",
    "anchors = []\n",
    "model = ct.Corex(n_hidden=6, seed=42)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = tfidf;\n",
    "y = df_col_merged.label.tolist()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7959468902865129"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: life, friend, people, thing, good, think, talk, way, love, person, care, happy, look, understand, hate\n",
      "Topic #2: start, day, panic, attack, week, month, doctor, ago, night, heart, experience, sleep, hour, chest, symptom\n",
      "Topic #3: try, find, help, bad, point, hard, read, thought, mind, mental, problem, post, feeling, struggle, write\n",
      "Topic #4: year, know, want, school, tell, end, relationship, old, break, away, college, anymore, ask, kid, lose\n",
      "Topic #5: time, work, come, need, lot, long, health, issue, new, able, use, right, situation, sure, high\n",
      "Topic #6: job, home, leave, live, family, pay, money, parent, house, stay, place, mom, let, room, walk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=15)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_1  topic_2  topic_3  topic_4  topic_5  topic_6\n",
       "0      0.0      1.0      0.0      1.0      0.0      0.0\n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "3      1.0      0.0      0.0      0.0      0.0      0.0\n",
       "4      1.0      0.0      0.0      1.0      0.0      0.0\n",
       "5      1.0      1.0      0.0      0.0      0.0      1.0\n",
       "6      1.0      1.0      1.0      1.0      1.0      1.0\n",
       "7      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "8      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "9      1.0      0.0      0.0      0.0      0.0      0.0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df = pd.DataFrame(\n",
    "    model.transform(tfidf), \n",
    "    columns=[\"topic_{}\".format(i+1) for i in range(6)]\n",
    ").astype(float)\n",
    "topic_df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors designed to nudge the model towards measuring specific genres\n",
    "anchors = [\n",
    "    [\"family\"],\n",
    "    [\"die\"],\n",
    "    [\"\"],\n",
    "    [\"\"],\n",
    "    [\"\"],\n",
    "    [\"\"],\n",
    "\n",
    "]\n",
    "anchors = [\n",
    "    [a for a in topic if a in vocab]\n",
    "    for topic in anchors\n",
    "]\n",
    "\n",
    "model = ct.Corex(n_hidden=8, seed=42)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab,\n",
    "    anchors=anchors, # Pass the anchors in here\n",
    "    anchor_strength=3 # Tell the model how much it should rely on the anchors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: relationship, year, friend, start, love, ago, month, girl, meet, break\n",
      "Topic #2: anymore, die, kill, want die, know anymore, want kill, anymore feel, anymore want, want anymore, fucking\n",
      "Topic #3: family, mom, dad, old, parent, friend family, year old, mother, brother, sister\n",
      "Topic #4: work, money, job, pay, work hard, bill, rent, school work, work feel, car\n",
      "Topic #5: play, enjoy, game, watch, video, video game, spend, play video, sit, movie\n",
      "Topic #6: school, motivation, high, college, lose, home, high school, class, able, anxiety\n",
      "Topic #7: time, thing, try, life, good, day, come, help, find, long\n",
      "Topic #8: know, think, people, want, tell, talk, feel like, bad, way, person\n"
     ]
    }
   ],
   "source": [
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17547</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17551 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_1  topic_2  topic_3  topic_4  topic_5  topic_6\n",
       "0          0.0      0.0      0.0      0.0      0.0      0.0\n",
       "1          0.0      0.0      0.0      0.0      0.0      0.0\n",
       "2          0.0      0.0      0.0      0.0      0.0      0.0\n",
       "3          0.0      0.0      0.0      0.0      1.0      0.0\n",
       "4          0.0      0.0      0.0      0.0      1.0      1.0\n",
       "...        ...      ...      ...      ...      ...      ...\n",
       "17546      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "17547      1.0      0.0      0.0      0.0      0.0      1.0\n",
       "17548      0.0      1.0      1.0      1.0      1.0      0.0\n",
       "17549      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "17550      1.0      1.0      1.0      1.0      1.0      1.0\n",
       "\n",
       "[17551 rows x 6 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df = pd.DataFrame(\n",
    "    model.transform(tfidf), \n",
    "    columns=[\"topic_{}\".format(i+1) for i in range(6)]\n",
    ").astype(float)\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--LDA UNSUPERVISED--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=6, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.107*\"\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.013*\"life\" + 0.009*\"love\" + 0.008*\"look\" + 0.007*\"play\" + '\n",
      "  '0.007*\"game\" + 0.005*\"world\" + 0.005*\"thing\" + 0.005*\"watch\" + '\n",
      "  '0.005*\"amp;#x200b\" + 0.004*\"video\" + 0.004*\"man\" + 0.004*\"good\" + '\n",
      "  '0.004*\"learn\" + 0.004*\"way\" + 0.004*\"people\" + 0.004*\"music\" + 0.004*\"time\" '\n",
      "  '+ 0.004*\"find\" + 0.003*\"remember\" + 0.003*\"year\" + 0.003*\"grow\" + '\n",
      "  '0.003*\"old\" + 0.003*\"dream\" + 0.003*\"read\" + 0.003*\"enjoy\" + 0.003*\"day\" + '\n",
      "  '0.003*\"face\" + 0.003*\"art\" + 0.003*\"use\" + 0.003*\"child\" + 0.003*\"mind\" + '\n",
      "  '0.003*\"self\" + 0.003*\"book\" + 0.003*\"come\" + 0.003*\"kid\" + 0.003*\"body\" + '\n",
      "  '0.002*\"awful\" + 0.002*\"believe\" + 0.002*\"human\" + 0.002*\"movie\" + '\n",
      "  '0.002*\"eye\" + 0.002*\"moment\" + 0.002*\"little\" + 0.002*\"woman\" + '\n",
      "  '0.002*\"hair\" + 0.002*\"spend\" + 0.002*\"lack\" + 0.002*\"tv\" + 0.002*\"create\" + '\n",
      "  '0.002*\"change\" + 0.002*\"realize\" + 0.002*\"beautiful\" + 0.002*\"live\" + '\n",
      "  '0.002*\"memory\" + 0.002*\"draw\" + 0.002*\"step\" + 0.002*\"big\" + 0.002*\"try\" + '\n",
      "  '0.002*\"like\" + 0.002*\"fat\" + 0.002*\"small\" + 0.002*\"walk\" + 0.002*\"let\" + '\n",
      "  '0.002*\"bring\" + 0.002*\"fear\" + 0.002*\"different\" + 0.002*\"run\" + '\n",
      "  '0.002*\"develop\" + 0.002*\"ugly\" + 0.002*\"skill\" + 0.002*\"hold\" + '\n",
      "  '0.002*\"head\" + 0.002*\"new\" + 0.002*\"hand\" + 0.002*\"weight\" + '\n",
      "  '0.002*\"experience\" + 0.002*\"person\" + 0.002*\"high\" + 0.002*\"share\" + '\n",
      "  '0.002*\"goal\" + 0.001*\"dog\" + 0.001*\"deep\" + 0.001*\"build\" + 0.001*\"age\" + '\n",
      "  '0.001*\"light\" + 0.001*\"far\" + 0.001*\"turn\" + 0.001*\"listen\" + '\n",
      "  '0.001*\"average\" + 0.001*\"character\" + 0.001*\"bear\" + 0.001*\"write\" + '\n",
      "  '0.001*\"fill\" + 0.001*\"society\" + 0.001*\"free\" + 0.001*\"heart\" + '\n",
      "  '0.001*\"mirror\" + 0.001*\"God\" + 0.001*\"eat\"'),\n",
      " (1,\n",
      "  '0.244*\"\\n'\n",
      "  ' \" + 0.136*\"  \" + 0.015*\" \\n'\n",
      "  ' \" + 0.011*\"   \" + 0.010*\"nan\" + 0.006*\" \\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.005*\"  \\n'\n",
      "  ' \" + 0.004*\"FUCK\" + 0.003*\"\\n'\n",
      "  '\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.003*\"fight\" + 0.003*\"gt;!pop!&lt\" + 0.003*\"clean\" + 0.003*\"room\" + '\n",
      "  '0.002*\"scream\" + 0.002*\"cop\" + 0.002*\"floor\" + 0.002*\"door\" + 0.002*\"water\" '\n",
      "  '+ 0.002*\" \\n'\n",
      "  '\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.002*\"\\n'\n",
      "  '\\n'\n",
      "  '\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.002*\"wash\" + 0.002*\"wait\" + 0.001*\"bathroom\" + 0.001*\"\\n'\n",
      "  '  \" + 0.001*\"blade\" + 0.001*\"tear\" + 0.001*\"run\" + 0.001*\"smell\" + '\n",
      "  '0.001*\"window\" + 0.001*\"eye\" + 0.001*\"face\" + 0.001*\"arm\" + 0.001*\"blood\" + '\n",
      "  '0.001*\"minute\" + 0.001*\"cold\" + 0.001*\"server\" + 0.001*\"hand\" + 0.001*\"red\" '\n",
      "  '+ 0.001*\"night\" + 0.001*\"HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH\" + 0.001*\"black\" '\n",
      "  '+ 0.001*\"let\" + 0.001*\"skin\" + 0.001*\"knife\" + 0.001*\"glass\" + 0.001*\"hit\" '\n",
      "  '+ 0.001*\"breeze\" + 0.001*\"dish\" + 0.001*\"\\n'\n",
      "  '\\n'\n",
      "  '   \" + 0.001*\"-i\" + 0.001*\"bath\" + 0.001*\"light\" + 0.001*\"food\" + '\n",
      "  '0.001*\"man\" + 0.001*\"Caroline\" + 0.001*\"ear\" + 0.001*\"body\" + 0.001*\"jump\" '\n",
      "  '+ 0.001*\"coma\" + 0.001*\"Maria\" + 0.001*\"refill\" + 0.001*\"clothe\" + '\n",
      "  '0.001*\"nose\" + 0.001*\"walk\" + 0.001*\"stair\" + 0.001*\"survey\" + 0.001*\"foot\" '\n",
      "  '+ 0.001*\"\\xa0\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.001*\"leave\" + 0.001*\"\\n'\n",
      "  '  \\n'\n",
      "  '  \\n'\n",
      "  ' \" + 0.001*\"theater\" + 0.001*\"lock\" + 0.001*\"lung\" + 0.001*\"wall\" + '\n",
      "  '0.001*\"vitamin\" + 0.001*\"kitchen\" + 0.001*\"nah\" + 0.001*\"breath\" + '\n",
      "  '0.001*\"2014\" + 0.001*\"tree\" + 0.001*\"open\" + 0.001*\"place\" + 0.001*\"acid\" + '\n",
      "  '0.001*\"green\" + 0.001*\"FUCKING\" + 0.001*\"wear\" + 0.001*\"smile\" + '\n",
      "  '0.001*\"milk\" + 0.001*\"GOD\" + 0.001*\"flower\" + 0.001*\"1/2\" + 0.001*\"blow\" + '\n",
      "  '0.001*\"flame\" + 0.001*\"counter\" + 0.001*\"truck\" + 0.001*\"beat\" + '\n",
      "  '0.001*\"stand\" + 0.001*\"rest\" + 0.001*\"rain\" + 0.001*\"neck\"'),\n",
      " (2,\n",
      "  '0.054*\"feel\" + 0.038*\"like\" + 0.027*\"want\" + 0.024*\"know\" + 0.020*\"life\" + '\n",
      "  '0.016*\"think\" + 0.013*\"thing\" + 0.013*\"\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.013*\"people\" + 0.011*\"try\" + 0.011*\"time\" + 0.010*\"day\" + '\n",
      "  '0.010*\"bad\" + 0.009*\"anymore\" + 0.008*\"way\" + 0.008*\"hate\" + 0.008*\"good\" + '\n",
      "  '0.007*\"help\" + 0.007*\"happy\" + 0.007*\"die\" + 0.007*\"live\" + '\n",
      "  '0.006*\"depression\" + 0.006*\"need\" + 0.006*\"care\" + 0.006*\"end\" + '\n",
      "  '0.006*\"thought\" + 0.006*\"love\" + 0.005*\"fuck\" + 0.005*\"fucking\" + '\n",
      "  '0.005*\"wish\" + 0.005*\"shit\" + 0.005*\"feeling\" + 0.005*\"right\" + '\n",
      "  '0.005*\"stop\" + 0.005*\"kill\" + 0.005*\"tired\" + 0.005*\"hard\" + '\n",
      "  '0.004*\"depressed\" + 0.004*\"point\" + 0.004*\"maybe\" + 0.004*\"hurt\" + '\n",
      "  '0.004*\"sleep\" + 0.004*\"sad\" + 0.004*\"person\" + 0.004*\"cry\" + 0.004*\"hope\" + '\n",
      "  '0.004*\"pain\" + 0.004*\"self\" + 0.004*\"happen\" + 0.004*\"world\" + 0.004*\"find\" '\n",
      "  '+ 0.004*\"lose\" + 0.004*\"away\" + 0.003*\"long\" + 0.003*\"mind\" + 0.003*\"tell\" '\n",
      "  '+ 0.003*\"come\" + 0.003*\"look\" + 0.003*\"understand\" + 0.003*\"head\" + '\n",
      "  '0.003*\"reason\" + 0.003*\"change\" + 0.003*\"matter\" + 0.003*\"actually\" + '\n",
      "  '0.003*\"start\" + 0.003*\"use\" + 0.003*\"leave\" + 0.003*\"work\" + 0.003*\"family\" '\n",
      "  '+ 0.002*\"suicide\" + 0.002*\"wake\" + 0.002*\"let\" + 0.002*\"  \" + 0.002*\"bed\" + '\n",
      "  '0.002*\"past\" + 0.002*\"wrong\" + 0.002*\"talk\" + 0.002*\"wanna\" + 0.002*\"mean\" '\n",
      "  '+ 0.002*\"able\" + 0.002*\"okay\" + 0.002*\"stupid\" + 0.002*\"normal\" + '\n",
      "  '0.002*\"suicidal\" + 0.002*\"deserve\" + 0.002*\"year\" + 0.002*\"guess\" + '\n",
      "  '0.002*\"honestly\" + 0.002*\"little\" + 0.002*\"problem\" + 0.002*\"sure\" + '\n",
      "  '0.002*\"everyday\" + 0.002*\"probably\" + 0.002*\"sorry\" + 0.002*\"scared\" + '\n",
      "  '0.002*\"inside\" + 0.002*\"lot\" + 0.002*\"emotion\" + 0.002*\"night\" + '\n",
      "  '0.002*\"alive\"'),\n",
      " (3,\n",
      "  '0.028*\"friend\" + 0.017*\"tell\" + 0.017*\"talk\" + 0.015*\"know\" + 0.015*\"\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.013*\"time\" + 0.013*\"like\" + 0.013*\"year\" + 0.012*\"want\" + '\n",
      "  '0.012*\"school\" + 0.010*\"people\" + 0.010*\"think\" + 0.009*\"start\" + '\n",
      "  '0.008*\"good\" + 0.008*\"parent\" + 0.007*\"thing\" + 0.007*\"mom\" + '\n",
      "  '0.007*\"family\" + 0.006*\"leave\" + 0.006*\"come\" + 0.006*\"try\" + 0.006*\"bad\" + '\n",
      "  '0.006*\"ask\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"relationship\" + '\n",
      "  '0.005*\"love\" + 0.005*\"home\" + 0.005*\"guy\" + 0.005*\"girl\" + 0.005*\"dad\" + '\n",
      "  '0.004*\"help\" + 0.004*\"old\" + 0.004*\"month\" + 0.004*\"break\" + 0.004*\"feel\" + '\n",
      "  '0.004*\"person\" + 0.004*\"meet\" + 0.004*\"post\" + 0.004*\"close\" + 0.004*\"need\" '\n",
      "  '+ 0.004*\"care\" + 0.003*\"house\" + 0.003*\"mother\" + 0.003*\"life\" + '\n",
      "  '0.003*\"problem\" + 0.003*\"find\" + 0.003*\"happen\" + 0.003*\"2\" + 0.003*\"ago\" + '\n",
      "  '0.003*\"girlfriend\" + 0.003*\"shit\" + 0.003*\"stuff\" + 0.003*\"week\" + '\n",
      "  '0.003*\"social\" + 0.003*\"kid\" + 0.003*\"cry\" + 0.003*\"brother\" + 0.003*\"end\" '\n",
      "  '+ 0.003*\"away\" + 0.003*\"room\" + 0.003*\"pretty\" + 0.003*\"hang\" + 0.003*\"use\" '\n",
      "  '+ 0.003*\"let\" + 0.003*\"sorry\" + 0.003*\"grade\" + 0.003*\"boyfriend\" + '\n",
      "  '0.003*\"date\" + 0.003*\"play\" + 0.002*\"reason\" + 0.002*\"look\" + '\n",
      "  '0.002*\"depression\" + 0.002*\"group\" + 0.002*\"new\" + 0.002*\"fuck\" + '\n",
      "  '0.002*\"stay\" + 0.002*\"high\" + 0.002*\"long\" + 0.002*\"3\" + 0.002*\"father\" + '\n",
      "  '0.002*\"sister\" + 0.002*\"cause\" + 0.002*\"night\" + 0.002*\"live\" + '\n",
      "  '0.002*\"actually\" + 0.002*\"probably\" + 0.002*\"fun\" + 0.002*\"stop\" + '\n",
      "  '0.002*\"nice\" + 0.002*\"later\" + 0.002*\"depressed\" + 0.002*\"wrong\" + '\n",
      "  '0.002*\"right\" + 0.002*\"hear\" + 0.002*\"ex\" + 0.002*\"sit\" + 0.002*\"mean\" + '\n",
      "  '0.002*\"hour\" + 0.002*\"text\"'),\n",
      " (4,\n",
      "  '0.163*\"remove\" + 0.031*\"u\" + 0.008*\"que\" + 0.007*\"ur\" + 0.006*\"de\" + '\n",
      "  '0.005*\"IM\" + 0.003*\"y\" + 0.003*\"la\" + 0.003*\"mi\" + 0.003*\"se\" + 0.003*\"en\" '\n",
      "  '+ 0.003*\"Dr\" + 0.003*\"lo\" + 0.002*\"idc\" + 0.002*\"con\" + 0.002*\"o\" + '\n",
      "  '0.002*\"worst\" + 0.002*\"barley\" + 0.002*\"silver\" + 0.002*\"por\" + '\n",
      "  '0.002*\"scissor\" + 0.002*\"stain\" + 0.002*\"los\" + 0.002*\"tantrum\" + '\n",
      "  '0.002*\"nunca\" + 0.002*\"xanax\" + 0.002*\"retard\" + 0.002*\"solitude\" + '\n",
      "  '0.002*\"corrupt\" + 0.002*\"command\" + 0.002*\"stimulus\" + 0.002*\"🖤\" + '\n",
      "  '0.002*\"una\" + 0.002*\"chris\" + 0.002*\"inappropriate\" + 0.002*\"curious\" + '\n",
      "  '0.002*\"el\" + 0.002*\"lump\" + 0.002*\"pero\" + 0.002*\"XD\" + 0.002*\"infected\" + '\n",
      "  '0.002*\"    \" + 0.002*\"supplement\" + 0.001*\"truely\" + 0.001*\"mod\" + '\n",
      "  '0.001*\"strategy\" + 0.001*\"anchor\" + 0.001*\"fluoxetine\" + 0.001*\"horrifying\" '\n",
      "  '+ 0.001*\"tengo\" + 0.001*\"Hope\" + 0.001*\"hover\" + 0.001*\"SAD\" + 0.001*\"es\" + '\n",
      "  '0.001*\"como\" + 0.001*\"hinder\" + 0.001*\"pig\" + 0.001*\"comforting\" + '\n",
      "  '0.001*\"appealing\" + 0.001*\"headphone\" + 0.001*\"versa\" + 0.001*\"slur\" + '\n",
      "  '0.001*\"siento\" + 0.001*\"organ\" + 0.001*\"barrel\" + 0.001*\"tape\" + '\n",
      "  '0.001*\"fiancee\" + 0.001*\"spectator\" + 0.001*\"lining\" + 0.001*\"salvation\" + '\n",
      "  '0.001*\"skilled\" + 0.001*\"nt\" + 0.001*\"contrary\" + 0.001*\"caretaker\" + '\n",
      "  '0.001*\"ya\" + 0.001*\"avece\" + 0.001*\"starvation\" + 0.001*\"Candy\" + '\n",
      "  '0.001*\"urself\" + 0.001*\"legacy\" + 0.001*\"walking\" + 0.001*\"greedy\" + '\n",
      "  '0.001*\"si\" + 0.001*\"carefully\" + 0.001*\"f*cke\" + 0.001*\"worm\" + '\n",
      "  '0.001*\"certainty\" + 0.001*\"CAN\\'T\" + 0.001*\"siempre\" + 0.001*\"cabinet\" + '\n",
      "  '0.001*\"outer\" + 0.001*\"specie\" + 0.001*\"le\" + 0.001*\"reincarnation\" + '\n",
      "  '0.001*\"ser\" + 0.001*\"repay\" + 0.001*\"HIV\" + 0.001*\"Pristiq\" + '\n",
      "  '0.001*\"sneeze\" + 0.001*\"mo\"'),\n",
      " (5,\n",
      "  '0.028*\"\\n'\n",
      "  '\\n'\n",
      "  ' \" + 0.022*\"work\" + 0.015*\"year\" + 0.015*\"job\" + 0.012*\"depression\" + '\n",
      "  '0.010*\"time\" + 0.010*\"week\" + 0.009*\"month\" + 0.008*\"day\" + 0.008*\"help\" + '\n",
      "  '0.008*\"start\" + 0.006*\"home\" + 0.006*\"mental\" + 0.006*\"anxiety\" + '\n",
      "  '0.006*\"health\" + 0.005*\"try\" + 0.005*\"live\" + 0.005*\"money\" + 0.004*\"need\" '\n",
      "  '+ 0.004*\"find\" + 0.004*\"college\" + 0.004*\"2\" + 0.004*\"therapist\" + '\n",
      "  '0.004*\"therapy\" + 0.004*\"pay\" + 0.004*\"struggle\" + 0.004*\"life\" + '\n",
      "  '0.004*\"medication\" + 0.004*\"ago\" + 0.004*\"3\" + 0.003*\"hour\" + 0.003*\"able\" '\n",
      "  '+ 0.003*\"virus\" + 0.003*\"doctor\" + 0.003*\"lose\" + 0.003*\"med\" + 0.003*\"new\" '\n",
      "  '+ 0.003*\"experience\" + 0.003*\"school\" + 0.003*\"bad\" + 0.003*\"lot\" + '\n",
      "  '0.003*\"long\" + 0.003*\"issue\" + 0.003*\"house\" + 0.003*\"come\" + 0.003*\"eat\" + '\n",
      "  '0.003*\"thing\" + 0.003*\"high\" + 0.003*\"family\" + 0.003*\"past\" + '\n",
      "  '0.003*\"stress\" + 0.003*\"class\" + 0.003*\"hard\" + 0.003*\"use\" + 0.003*\"sleep\" '\n",
      "  '+ 0.003*\"stay\" + 0.002*\"diagnose\" + 0.002*\"parent\" + 0.002*\"know\" + '\n",
      "  '0.002*\"spend\" + 0.002*\"leave\" + 0.002*\"plan\" + 0.002*\"study\" + '\n",
      "  '0.002*\"quarantine\" + 0.002*\"situation\" + 0.002*\"place\" + 0.002*\"fail\" + '\n",
      "  '0.002*\"bed\" + 0.002*\"online\" + 0.002*\"5\" + 0.002*\"4\" + 0.002*\"etc\" + '\n",
      "  '0.002*\"6\" + 0.002*\"state\" + 0.002*\"social\" + 0.002*\"finally\" + '\n",
      "  '0.002*\"problem\" + 0.002*\"food\" + 0.002*\"good\" + 0.002*\"quit\" + '\n",
      "  '0.002*\"point\" + 0.002*\"right\" + 0.002*\"deal\" + 0.002*\"currently\" + '\n",
      "  '0.002*\"10\" + 0.002*\"major\" + 0.002*\"car\" + 0.002*\"hospital\" + '\n",
      "  '0.002*\"support\" + 0.002*\"psychiatrist\" + 0.002*\"drug\" + 0.002*\"graduate\" + '\n",
      "  '0.002*\"sure\" + 0.002*\"university\" + 0.002*\"severe\" + 0.002*\"advice\" + '\n",
      "  '0.002*\"think\" + 0.002*\"symptom\" + 0.002*\"today\" + 0.002*\"thank\"')]\n"
     ]
    }
   ],
   "source": [
    "#print the keyword in the 10 topics\n",
    "pprint(lda_model.print_topics(num_words=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[Uk, cahms, service, shit, sure, schizophrenia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>\\n\\n , work, year, job, depression, time, week...</td>\n",
       "      <td>[anybody, luck, holistic, treatment, gut, heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[leave, bed, 4:20pm, work, 5:00pm, feel, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[want, help, people, care, feel, right, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[obviously, want, unhappy, want, depressed, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[remove]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4786</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[snapchat, caption, lol, https://i.imgur.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3856</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[grandma, 300, $, acoustic, guitar, plug, amp,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5771</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[know, feel, right, drug, wander, guy, deal, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0             0             3.0              0.3478   \n",
       "1             1             5.0              0.4429   \n",
       "2             2             2.0              0.7067   \n",
       "3             3             2.0              0.6437   \n",
       "4             4             2.0              0.9316   \n",
       "..          ...             ...                 ...   \n",
       "95           95             2.0              0.3870   \n",
       "96           96             2.0              0.4786   \n",
       "97           97             3.0              0.3257   \n",
       "98           98             3.0              0.3856   \n",
       "99           99             2.0              0.5771   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "1   \\n\\n , work, year, job, depression, time, week...   \n",
       "2   feel, like, want, know, life, think, thing, \\n...   \n",
       "3   feel, like, want, know, life, think, thing, \\n...   \n",
       "4   feel, like, want, know, life, think, thing, \\n...   \n",
       "..                                                ...   \n",
       "95  feel, like, want, know, life, think, thing, \\n...   \n",
       "96  feel, like, want, know, life, think, thing, \\n...   \n",
       "97  friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "98  friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "99  feel, like, want, know, life, think, thing, \\n...   \n",
       "\n",
       "                                                 Text  \n",
       "0   [Uk, cahms, service, shit, sure, schizophrenia...  \n",
       "1   [anybody, luck, holistic, treatment, gut, heal...  \n",
       "2   [leave, bed, 4:20pm, work, 5:00pm, feel, like,...  \n",
       "3   [want, help, people, care, feel, right, like, ...  \n",
       "4   [obviously, want, unhappy, want, depressed, mi...  \n",
       "..                                                ...  \n",
       "95                                           [remove]  \n",
       "96                                                 []  \n",
       "97  [snapchat, caption, lol, https://i.imgur.com/s...  \n",
       "98  [grandma, 300, $, acoustic, guitar, plug, amp,...  \n",
       "99  [know, feel, right, drug, wander, guy, deal, s...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=doc_list):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=doc_list)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[Uk, cahms, service, shit, sure, schizophrenia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>\\n\\n , work, year, job, depression, time, week...</td>\n",
       "      <td>[anybody, luck, holistic, treatment, gut, heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[leave, bed, 4:20pm, work, 5:00pm, feel, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[want, help, people, care, feel, right, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[obviously, want, unhappy, want, depressed, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4599</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[watch, countless, reddit, video, topic, creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3061</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[know, write, feel, like, place, thing, moment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8425</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[society, fucking, want, dead, wish, honestly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4307</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[\\n , 26, yo, \\n\\n , ready, honestly, end, hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[Indica, edible, Wednesday, feel, amazing, usu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>\\n\\n , work, year, job, depression, time, week...</td>\n",
       "      <td>[dear, Reddit, \\n\\n , sophomore, majoring, che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3684</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[happy,   , steady, decently, pay, job, wife, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6524</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[idk, pretty, easy,   \\n\\n\\n , want, like, dra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[hi, recently, family, dog, die, jan, 25, day,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6901</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[use, feel, lonely, night, usually, 20h/21h, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4559</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[genuinely, want, die, work, Hobbies, social, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6696</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[approach, people, talk, introvert, try, frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6969</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[recently, break, girlfriend, tell, friend, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4503</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[battle, depression, life, \\n\\n , 1-foot, stoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7587</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[understand, purpose, feel, like, world, forge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5753</td>\n",
       "      <td>friend, tell, talk, know, \\n\\n , time, like, y...</td>\n",
       "      <td>[week, bad, depressive, episode, girlfriend, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7194</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[feel, like, long, truly, care, want, spend, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6693</td>\n",
       "      <td>feel, like, want, know, life, think, thing, \\n...</td>\n",
       "      <td>[feel, tired, non, stop, november, remember, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>\\n\\n , work, year, job, depression, time, week...</td>\n",
       "      <td>[Anonymous, 9, question, 2, 4, min, complete, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0             0             3.0              0.3478   \n",
       "1             1             5.0              0.4429   \n",
       "2             2             2.0              0.7067   \n",
       "3             3             2.0              0.6437   \n",
       "4             4             2.0              0.9316   \n",
       "5             5             2.0              0.4599   \n",
       "6             6             2.0              0.3061   \n",
       "7             7             2.0              0.8425   \n",
       "8             8             2.0              0.4307   \n",
       "9             9             2.0              0.7250   \n",
       "10           10             5.0              0.4115   \n",
       "11           11             2.0              0.3684   \n",
       "12           12             2.0              0.6524   \n",
       "13           13             3.0              0.5246   \n",
       "14           14             2.0              0.6901   \n",
       "15           15             3.0              0.4559   \n",
       "16           16             2.0              0.6696   \n",
       "17           17             3.0              0.6969   \n",
       "18           18             2.0              0.4503   \n",
       "19           19             2.0              0.7587   \n",
       "20           20             2.0              0.3870   \n",
       "21           21             3.0              0.5753   \n",
       "22           22             2.0              0.7194   \n",
       "23           23             2.0              0.6693   \n",
       "24           24             5.0              0.9198   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "1   \\n\\n , work, year, job, depression, time, week...   \n",
       "2   feel, like, want, know, life, think, thing, \\n...   \n",
       "3   feel, like, want, know, life, think, thing, \\n...   \n",
       "4   feel, like, want, know, life, think, thing, \\n...   \n",
       "5   feel, like, want, know, life, think, thing, \\n...   \n",
       "6   feel, like, want, know, life, think, thing, \\n...   \n",
       "7   feel, like, want, know, life, think, thing, \\n...   \n",
       "8   feel, like, want, know, life, think, thing, \\n...   \n",
       "9   feel, like, want, know, life, think, thing, \\n...   \n",
       "10  \\n\\n , work, year, job, depression, time, week...   \n",
       "11  feel, like, want, know, life, think, thing, \\n...   \n",
       "12  feel, like, want, know, life, think, thing, \\n...   \n",
       "13  friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "14  feel, like, want, know, life, think, thing, \\n...   \n",
       "15  friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "16  feel, like, want, know, life, think, thing, \\n...   \n",
       "17  friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "18  feel, like, want, know, life, think, thing, \\n...   \n",
       "19  feel, like, want, know, life, think, thing, \\n...   \n",
       "20  feel, like, want, know, life, think, thing, \\n...   \n",
       "21  friend, tell, talk, know, \\n\\n , time, like, y...   \n",
       "22  feel, like, want, know, life, think, thing, \\n...   \n",
       "23  feel, like, want, know, life, think, thing, \\n...   \n",
       "24  \\n\\n , work, year, job, depression, time, week...   \n",
       "\n",
       "                                                 Text  \n",
       "0   [Uk, cahms, service, shit, sure, schizophrenia...  \n",
       "1   [anybody, luck, holistic, treatment, gut, heal...  \n",
       "2   [leave, bed, 4:20pm, work, 5:00pm, feel, like,...  \n",
       "3   [want, help, people, care, feel, right, like, ...  \n",
       "4   [obviously, want, unhappy, want, depressed, mi...  \n",
       "5   [watch, countless, reddit, video, topic, creat...  \n",
       "6   [know, write, feel, like, place, thing, moment...  \n",
       "7   [society, fucking, want, dead, wish, honestly,...  \n",
       "8   [\\n , 26, yo, \\n\\n , ready, honestly, end, hel...  \n",
       "9   [Indica, edible, Wednesday, feel, amazing, usu...  \n",
       "10  [dear, Reddit, \\n\\n , sophomore, majoring, che...  \n",
       "11  [happy,   , steady, decently, pay, job, wife, ...  \n",
       "12  [idk, pretty, easy,   \\n\\n\\n , want, like, dra...  \n",
       "13  [hi, recently, family, dog, die, jan, 25, day,...  \n",
       "14  [use, feel, lonely, night, usually, 20h/21h, s...  \n",
       "15  [genuinely, want, die, work, Hobbies, social, ...  \n",
       "16  [approach, people, talk, introvert, try, frien...  \n",
       "17  [recently, break, girlfriend, tell, friend, ha...  \n",
       "18  [battle, depression, life, \\n\\n , 1-foot, stoo...  \n",
       "19  [understand, purpose, feel, like, world, forge...  \n",
       "20                                              [nan]  \n",
       "21  [week, bad, depressive, episode, girlfriend, d...  \n",
       "22  [feel, like, long, truly, care, want, spend, t...  \n",
       "23  [feel, tired, non, stop, november, remember, l...  \n",
       "24  [Anonymous, 9, question, 2, 4, min, complete, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic.head(25)\n",
    "#df_dominant_topic.to_csv(r'depression_2020_datest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3.0: 3007, 5.0: 2454, 2.0: 13603, 0.0: 301, 1.0: 97, 4.0: 18})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(df_dominant_topic.Dominant_Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newest_doc = newest_doc[newest_doc.Body != '[removed]'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Uk CAHMS service is shit. I’m sure I have schi...\n",
       "1        Has anybody had any luck with holistic treatme...\n",
       "2        Didn’t leave bed until 4:20pm so I could go to...\n",
       "3        I want help, I can't say to the people that ca...\n",
       "4        Obviously I don't want to be unhappy. I don't ...\n",
       "                               ...                        \n",
       "19474    I finally open up to my therapist about how I ...\n",
       "19475    I need to go back to work... it takes over mos...\n",
       "19477    THEORY OF MASKS\\n\\nSo what do i think when i v...\n",
       "19478    I have no one. I've lost pretty much everythin...\n",
       "19479    Long story short, my life is hard enough. A ro...\n",
       "Name: Body, Length: 17285, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newest_doc.Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Uk cahms service shit sure schizophrenia fucki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>anybody luck holistic treatment gut health met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>leave bed 4:20pm work 5:00pm feel like walk dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>want help people care feel right like wrong bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>obviously want unhappy want depressed miserabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19475</td>\n",
       "      <td>need work day hate work hate wake early hate c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19476</td>\n",
       "      <td>remove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19477</td>\n",
       "      <td>THEORY MASKS \\n\\n  think visualize mask Mask p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19478</td>\n",
       "      <td>lose pretty cry sorry mom dad longer second pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19479</td>\n",
       "      <td>long story short life hard rough gist mental a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19480 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Body\n",
       "0      Uk cahms service shit sure schizophrenia fucki...\n",
       "1      anybody luck holistic treatment gut health met...\n",
       "2      leave bed 4:20pm work 5:00pm feel like walk dr...\n",
       "3      want help people care feel right like wrong bo...\n",
       "4      obviously want unhappy want depressed miserabl...\n",
       "...                                                  ...\n",
       "19475  need work day hate work hate wake early hate c...\n",
       "19476                                             remove\n",
       "19477  THEORY MASKS \\n\\n  think visualize mask Mask p...\n",
       "19478  lose pretty cry sorry mom dad longer second pa...\n",
       "19479  long story short life hard rough gist mental a...\n",
       "\n",
       "[19480 rows x 1 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16731\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: like, know, friend, think, people, thing, try, good, talk, feel like, tell, bad, person, way, love\n",
      "Topic #2: year, job, school, work, start, old, find, parent, college, high, year old, money, ago, high school, new\n",
      "Topic #3: time, day, come, home, leave, need, long, cry, away, night, hour, sleep, hard, break, bed\n",
      "Topic #4: use, look, lose, play, game, social, world, change, mind, watch, girl, video, grow, real, video game\n",
      "Topic #5: depression, help, mental, anxiety, month, health, week, mental health, issue, struggle, self, thought, diagnose, therapy, therapist\n",
      "Topic #6: life, want, live, anymore, die, end, hate, shit, point, fucking, fuck, kill, stop, wish\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-415b03f8f5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m ).astype(float)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "topic_df = pd.DataFrame(\n",
    "    model.transform(tfidf), \n",
    "    columns=[\"topic_{}\".format(i+1) for i in range(6)]\n",
    ").astype(float)\n",
    "\n",
    "df = pd.concat([df, topic_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17547</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17551 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_1  topic_2  topic_3  topic_4  topic_5  topic_6\n",
       "0          0.0      0.0      0.0      0.0      0.0      0.0\n",
       "1          0.0      0.0      0.0      0.0      0.0      0.0\n",
       "2          0.0      0.0      0.0      0.0      0.0      0.0\n",
       "3          0.0      0.0      0.0      0.0      1.0      0.0\n",
       "4          0.0      0.0      0.0      0.0      1.0      1.0\n",
       "...        ...      ...      ...      ...      ...      ...\n",
       "17546      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "17547      1.0      0.0      0.0      0.0      0.0      1.0\n",
       "17548      0.0      1.0      1.0      1.0      1.0      0.0\n",
       "17549      0.0      0.0      0.0      0.0      0.0      0.0\n",
       "17550      1.0      1.0      1.0      1.0      1.0      1.0\n",
       "\n",
       "[17551 rows x 6 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
