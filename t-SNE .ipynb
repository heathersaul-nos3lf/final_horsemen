{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_lg\n",
    "\n",
    "from pprint import pprint\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import tkinter\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import yellowbrick\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define nlp and start cleaning data\n",
    "nlp= spacy.load(\"en\")\n",
    "\n",
    "# List of stop words to equalize data\n",
    "stop_list = [\"Depression\",\"depression\"]\n",
    "\n",
    "# Updates spaCy's default stop words list with my additional words. \n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Depression</td>\n",
       "      <td>1583020847</td>\n",
       "      <td>jamie11w</td>\n",
       "      <td>Got referred to volunteers</td>\n",
       "      <td>Uk CAHMS service is shit. I’m sure I have schi...</td>\n",
       "      <td>/r/depression/comments/fblivc/got_referred_to_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Depression</td>\n",
       "      <td>1583020904</td>\n",
       "      <td>NoDeedUnpunished</td>\n",
       "      <td>Any luck with holistic solutions?</td>\n",
       "      <td>Has anybody had any luck with holistic treatme...</td>\n",
       "      <td>/r/depression/comments/fbljcl/any_luck_with_ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Depression</td>\n",
       "      <td>1583021169</td>\n",
       "      <td>Fancy_Pens</td>\n",
       "      <td>Slept the day away again</td>\n",
       "      <td>Didn’t leave bed until 4:20pm so I could go to...</td>\n",
       "      <td>/r/depression/comments/fbllbk/slept_the_day_aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Depression</td>\n",
       "      <td>1583021314</td>\n",
       "      <td>creeklisa</td>\n",
       "      <td>getting deeper and deeper</td>\n",
       "      <td>I want help, I can't say to the people that ca...</td>\n",
       "      <td>/r/depression/comments/fblmhb/getting_deeper_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Depression</td>\n",
       "      <td>1583021438</td>\n",
       "      <td>JudasofBelial</td>\n",
       "      <td>I want it to be okay for me to be unhappy.</td>\n",
       "      <td>Obviously I don't want to be unhappy. I don't ...</td>\n",
       "      <td>/r/depression/comments/fblngh/i_want_it_to_be_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7149</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>1585612200</td>\n",
       "      <td>Prestoric_user</td>\n",
       "      <td>I want to sell my console but its hard</td>\n",
       "      <td>I want to sell my ps4 since i dont use it anym...</td>\n",
       "      <td>/r/Anxiety/comments/fs251x/i_want_to_sell_my_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>1585612297</td>\n",
       "      <td>Gametest014</td>\n",
       "      <td>Just looking for advice</td>\n",
       "      <td>So yesterday(this morning) I went to sleep at ...</td>\n",
       "      <td>/r/Anxiety/comments/fs260r/just_looking_for_ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7151</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>1585612474</td>\n",
       "      <td>throw-away-3005</td>\n",
       "      <td>My first bath</td>\n",
       "      <td>I took the first bath in our new house and it ...</td>\n",
       "      <td>/r/Anxiety/comments/fs27t3/my_first_bath/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7152</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>1585612576</td>\n",
       "      <td>AnxiousFishermen</td>\n",
       "      <td>Anyone else always crossing their legs?</td>\n",
       "      <td>Without meaning too i always find my self cros...</td>\n",
       "      <td>/r/Anxiety/comments/fs28s1/anyone_else_always_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7153</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>1585612638</td>\n",
       "      <td>LoCoSZN</td>\n",
       "      <td>My anxiety keeps coming after me every single ...</td>\n",
       "      <td>Ever since the start of online classes it’s be...</td>\n",
       "      <td>/r/Anxiety/comments/fs29e4/my_anxiety_keeps_co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28097 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Subreddit        Date            Author  \\\n",
       "0     Depression  1583020847          jamie11w   \n",
       "1     Depression  1583020904  NoDeedUnpunished   \n",
       "2     Depression  1583021169        Fancy_Pens   \n",
       "3     Depression  1583021314         creeklisa   \n",
       "4     Depression  1583021438     JudasofBelial   \n",
       "...          ...         ...               ...   \n",
       "7149     Anxiety  1585612200    Prestoric_user   \n",
       "7150     Anxiety  1585612297       Gametest014   \n",
       "7151     Anxiety  1585612474   throw-away-3005   \n",
       "7152     Anxiety  1585612576  AnxiousFishermen   \n",
       "7153     Anxiety  1585612638           LoCoSZN   \n",
       "\n",
       "                                                  Title  \\\n",
       "0                            Got referred to volunteers   \n",
       "1                     Any luck with holistic solutions?   \n",
       "2                              Slept the day away again   \n",
       "3                             getting deeper and deeper   \n",
       "4            I want it to be okay for me to be unhappy.   \n",
       "...                                                 ...   \n",
       "7149             I want to sell my console but its hard   \n",
       "7150                            Just looking for advice   \n",
       "7151                                      My first bath   \n",
       "7152            Anyone else always crossing their legs?   \n",
       "7153  My anxiety keeps coming after me every single ...   \n",
       "\n",
       "                                                   Body  \\\n",
       "0     Uk CAHMS service is shit. I’m sure I have schi...   \n",
       "1     Has anybody had any luck with holistic treatme...   \n",
       "2     Didn’t leave bed until 4:20pm so I could go to...   \n",
       "3     I want help, I can't say to the people that ca...   \n",
       "4     Obviously I don't want to be unhappy. I don't ...   \n",
       "...                                                 ...   \n",
       "7149  I want to sell my ps4 since i dont use it anym...   \n",
       "7150  So yesterday(this morning) I went to sleep at ...   \n",
       "7151  I took the first bath in our new house and it ...   \n",
       "7152  Without meaning too i always find my self cros...   \n",
       "7153  Ever since the start of online classes it’s be...   \n",
       "\n",
       "                                                    URL  \n",
       "0     /r/depression/comments/fblivc/got_referred_to_...  \n",
       "1     /r/depression/comments/fbljcl/any_luck_with_ho...  \n",
       "2     /r/depression/comments/fbllbk/slept_the_day_aw...  \n",
       "3     /r/depression/comments/fblmhb/getting_deeper_a...  \n",
       "4     /r/depression/comments/fblngh/i_want_it_to_be_...  \n",
       "...                                                 ...  \n",
       "7149  /r/Anxiety/comments/fs251x/i_want_to_sell_my_c...  \n",
       "7150  /r/Anxiety/comments/fs260r/just_looking_for_ad...  \n",
       "7151          /r/Anxiety/comments/fs27t3/my_first_bath/  \n",
       "7152  /r/Anxiety/comments/fs28s1/anyone_else_always_...  \n",
       "7153  /r/Anxiety/comments/fs29e4/my_anxiety_keeps_co...  \n",
       "\n",
       "[28097 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging csv files (without repeating header)\n",
    "interesting_files = glob.glob(\"*.csv\") \n",
    "df = pd.concat((pd.read_csv(f, header = 0) for f in interesting_files))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of documents (list of lists) \n",
    "text_doc = df['Body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in text_doc:\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(str(doc))\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Uk cahms service shit sure schizophrenia fucki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>anybody luck holistic treatment gut health met...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>leave bed 4:20pm work 5:00pm feel like walk dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>want help people care feel right like wrong bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>obviously want unhappy want depressed miserabl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28092</td>\n",
       "      <td>want sell ps4 use anymore want money hard pers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28093</td>\n",
       "      <td>yesterday(this morning sleep midnight 12 wake ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28094</td>\n",
       "      <td>bath new house cause flood basement live boyfr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28095</td>\n",
       "      <td>mean find self cross leg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28096</td>\n",
       "      <td>start online class day day anxiety control ann...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28097 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Body  label\n",
       "0      Uk cahms service shit sure schizophrenia fucki...      0\n",
       "1      anybody luck holistic treatment gut health met...      0\n",
       "2      leave bed 4:20pm work 5:00pm feel like walk dr...      0\n",
       "3      want help people care feel right like wrong bo...      0\n",
       "4      obviously want unhappy want depressed miserabl...      0\n",
       "...                                                  ...    ...\n",
       "28092  want sell ps4 use anymore want money hard pers...      0\n",
       "28093  yesterday(this morning sleep midnight 12 wake ...      0\n",
       "28094  bath new house cause flood basement live boyfr...      0\n",
       "28095                           mean find self cross leg      0\n",
       "28096  start online class day day anxiety control ann...      0\n",
       "\n",
       "[28097 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#However doc_list gives us each post as a list, with individual words being elements \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "empt = [];\n",
    "for doc in doc_list:\n",
    "    a1 = TreebankWordDetokenizer().detokenize(doc)\n",
    "    empt.append(a1)\n",
    "    \n",
    "df_doc_dep = pd.DataFrame(empt,columns = ['Body'])\n",
    "#df_doc_dep is a dataframe that has cleaned posts from Depression subreddit.\n",
    "#all the 'removed' posts are gone and all the stopwords in the individual posts are gone! \n",
    "\n",
    "#both doc_list and df_doc are important (at least I think so :D)\n",
    "\n",
    "\n",
    "df_doc_dep['label'] = 0\n",
    "df_doc_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=.2,\n",
    "    min_df=8,\n",
    "    max_features=None,\n",
    "    ngram_range=(1, 1),\n",
    "    norm=None,\n",
    "    binary=True,\n",
    "    use_idf=True,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "\n",
    "vectorizer = vectorizer.fit(df_doc_dep.Body)\n",
    "tfidf = vectorizer.transform(df_doc_dep.Body)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.offline as plt\n",
    "import plotly.graph_objs as go\n",
    "plt.init_notebook_mode(connected=True)\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-020627680290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1547\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mbinary_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TO_BINARY_LUT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mdecompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TODO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/smart_open/local_file.py\u001b[0m in \u001b[0;36mopen_uri\u001b[0;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mparsed_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uri_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(model, words):\n",
    "    arr = np.empty((0,300), dtype='f')\n",
    "    labels = []\n",
    "    for wrd_score in words:\n",
    "        try:\n",
    "            wrd_vector = model[wrd_score]\n",
    "            arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "            labels.append(wrd_score)\n",
    "        except:\n",
    "            pass\n",
    "    tsne = TSNE(n_components=3, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    z_coords = Y[:, 2]\n",
    "    return x_coords, y_coords, z_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ok_computer.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "words = np.unique(data.replace('\\n', '').replace(',', ' ').replace('(','').replace(')','').split(' '))\n",
    "ok_computer = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "x, y, z = get_coordinates(model, ok_computer)\n",
    "\n",
    "plot = [go.Scatter3d(x = x,\n",
    "                    y = y,\n",
    "                    z = z,\n",
    "                    mode = 'markers+text',\n",
    "                    text = words,\n",
    "                    textposition='bottom center',\n",
    "                    hoverinfo = 'text',\n",
    "                    marker=dict(size=5,opacity=0.8))]\n",
    "\n",
    "layout = go.Layout(title='Ok Computer lyrics')\n",
    "fig = go.Figure(data=plot, layout=layout)\n",
    "plt.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "\n",
    "\n",
    "# Load the data and create document vectors\n",
    "corpus = df_doc_dep.Body\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X = tfidf.fit_transform(corpus)\n",
    "y = new_corpus1.label\n",
    "\n",
    "# Create the visualizer and draw the vectors\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(X, y)\n",
    "tsne.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
