{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Install libraries--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install newspaper3k\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install seaborn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Load dependencies--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "import newspaper\n",
    "from newspaper import article\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from textblob import TextBlob\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Read in CSV File--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv (r'test_agor.csv')\n",
    "new_df = df[['Title', 'Body']] \n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Remove punctuation and stopwords--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "useless_words = nltk.corpus.stopwords.words(\"english\") + list(punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Remove additional \"noisy\" text--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words_extended = useless_words.copy()\n",
    "useless_words_extended.extend([\"n't\",\"'m\",\"'s\", \"'ve\", \"'ll\", \"'re\", \"...\", \"..\", '``', \"''\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Create the raw corpus--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = new_df['Title'].to_list()\n",
    "b = new_df['Body'].to_list()\n",
    "corpus = [''.join(map(str, i)) for i in zip(a, b)]\n",
    "\n",
    "# loop through each element in the list (in this case the list made of string elements) and tokenize each line and add it to the list of words\n",
    "list_of_words = []\n",
    "for line in corpus:\n",
    "    list_of_words.extend(nltk.word_tokenize(line.lower()))\n",
    "list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Apply code to clean corpus--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word for word in list_of_words if not word in useless_words_extended]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--obtain word counts--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter(filtered_words)\n",
    "word_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--lemmatize data--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "lemmatized_wordslist = []\n",
    "for word in filtered_words:\n",
    "    lem_str = lemmatizer.lemmatize(word)\n",
    "    lemmatized_wordslist.append(lem_str)\n",
    "\n",
    "lemmatized_wordslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--pass cleaned and lemmatized data through spaCy nlp pipeline--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "cleaned_wordlist = nlp(str(lemmatized_wordslist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--count adjectives in the data--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [[token,token.pos_] for token in cleaned_wordlist]\n",
    "tk = pd.DataFrame(token_list,columns = ['word','POS'])\n",
    "tk = tk[tk.POS != 'PUNCT']\n",
    "tk_adj = tk[tk.POS == 'ADJ']\n",
    "\n",
    "l1 = [g1.text for g1 in tk_adj.word.tolist()]\n",
    "d =Counter(l1)\n",
    "agora_adj_df = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "agora_adj_df.columns = ['word','count']\n",
    "\n",
    "agora_adj_df['POS'] =pd.Series(['ADJ' for i in range(agora_adj_df.shape[0])])\n",
    "agora_adj_df = agora_adj_df.sort_values(by = 'count', ascending = False)\n",
    "agora_adj_df.iloc[0:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--count nouns  in the data--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = tk[tk.POS != 'PUNCT']\n",
    "tk_noun = tk[tk.POS == 'NOUN']\n",
    "l1 = [g1.text for g1 in tk_noun.word.tolist()]\n",
    "d =Counter(l1)\n",
    "agora_noun_df = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "agora_noun_df.columns = ['word','count']\n",
    "\n",
    "agora_noun_df['POS'] =pd.Series(['NOUN' for i in range(agora_noun_df.shape[0])])\n",
    "agora_noun_df = agora_noun_df.sort_values(by = 'count', ascending = False)\n",
    "agora_noun_df.iloc[0:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--count verbs in the data--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [g1.text for g1 in tk_verb.word.tolist()]\n",
    "d =Counter(l1)\n",
    "agora_verb_df = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "agora_verb_df.columns = ['word','count']\n",
    "agora_verb_df.sort_values(by='count', ascending=False)\n",
    "agora_verb_df['POS'] =pd.Series(['VERB' for i in range(agora_verb_df.shape[0])])\n",
    "agora_verb_df = agora_verb_df.sort_values(by = 'count', ascending = False)\n",
    "agora_verb_df.iloc[:45]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--get and count trigrams in the data--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(clean_doc)\n",
    "blob3 = blob.ngrams(n=3)\n",
    "flat_blob3 = [bb[0]+' '+bb[1]+' ' +bb[2] for bb in blob3]\n",
    "Counter(flat_blob3).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--calculate tfidf for unigrams--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(corpus)\n",
    "# print(word_count_vector)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(zip(cv.get_feature_names(),tfidf_transformer.idf_), columns=[\"word\",\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by='idf_weights',ascending=False)\n",
    "\n",
    "df_idf.to_csv('test_idf.csv')\n",
    "df_idf.sort_values(by='idf_weights',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "# just send in all your docs here\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document_vector = tfidf_vectorizer_vectors[0]\n",
    "df = pd.DataFrame(first_document_vector.T.todense(),  columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
